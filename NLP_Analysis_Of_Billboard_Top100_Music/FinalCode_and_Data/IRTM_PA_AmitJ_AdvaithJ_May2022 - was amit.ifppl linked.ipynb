{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mounting Gdrive, any necessary packages and routine imports"
      ],
      "metadata": {
        "id": "YcL4oKTXNg2c"
      },
      "id": "YcL4oKTXNg2c"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "# Import modules \n",
        "%matplotlib inline\n",
        "!pip install pymysql\n",
        "!pip install twython\n",
        "!pip install langdetect\n",
        "!pip install empath\n",
        "!pip install textblob\n",
        "!pip install vaderSentiment\n",
        "!pip install gensim\n",
        "!pip install pyLDAvis\n",
        "!pip install bertopic[all]\n",
        "!pip install transformers -q"
      ],
      "metadata": {
        "id": "s-hAcESSZSri"
      },
      "id": "s-hAcESSZSri",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "956cabd0-5da4-4e5b-b860-b320c6239822",
      "metadata": {
        "id": "956cabd0-5da4-4e5b-b860-b320c6239822"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import patsy\n",
        "import statsmodels.api as sm\n",
        "from scipy.stats import ttest_ind\n",
        "from numpy.random import rand\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from langdetect import detect\n",
        "from bs4 import Tag, NavigableString, BeautifulSoup\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "from nltk.sentiment.util import *\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk import pos_tag\n",
        "from nltk import word_tokenize, wordpunct_tokenize, sent_tokenize, tokenize\n",
        "from nltk.tag.stanford import StanfordNERTagger\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.models import TfidfModel\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "import os\n",
        "from sklearn.decomposition import PCA\n",
        "from transformers import pipeline\n",
        "from bertopic import BERTopic\n",
        "import spacy\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models\n",
        "from sqlalchemy import create_engine\n",
        "engine = create_engine('mysql+pymysql://admin:DveniTfic1NKHI9HluJ3@project.ccmd1auuwzyh.us-east-1.rds.amazonaws.com/project')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Scraped Data from SQL db"
      ],
      "metadata": {
        "id": "XNFoIo3_m-7J"
      },
      "id": "XNFoIo3_m-7J"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e28491c-d256-4087-be70-e9b7016180d9",
      "metadata": {
        "id": "3e28491c-d256-4087-be70-e9b7016180d9"
      },
      "outputs": [],
      "source": [
        "lyrics_df = pd.read_sql('select * from BILLBOARD',engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "ppXLRj9lNqxg"
      },
      "id": "ppXLRj9lNqxg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lyrics Extraction from HTML and Regex Cleaning to deal with informal language of songs"
      ],
      "metadata": {
        "id": "k03DDACunJ33"
      },
      "id": "k03DDACunJ33"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60342c4e-d20b-4760-9450-030772b30dd5",
      "metadata": {
        "id": "60342c4e-d20b-4760-9450-030772b30dd5"
      },
      "outputs": [],
      "source": [
        "def extract_lyrics(x):\n",
        "    lyrics = []\n",
        "    tags = BeautifulSoup(x).find_all('div',class_ = re.compile('Lyrics__Container(.*)'))\n",
        "    for i in range(len(tags)):\n",
        "        relevant_tags = tags[i].find_all('br')\n",
        "        extracted_tags = [x.nextSibling for x in relevant_tags]\n",
        "        for tag in extracted_tags:\n",
        "            if(type(tag) is NavigableString):\n",
        "                lyrics.append('. '.join([str(tag)]) + ' .')\n",
        "            if(type(tag) is Tag): \n",
        "                rec_tag = tag.text\n",
        "                # rec_tags = [x.nextSibling for x in rec_tags]\n",
        "                # for rec_tag in rec_tags:\n",
        "                #     if(type(rec_tag) is NavigableString):\n",
        "                lyrics.append('. '.join([str(rec_tag)])+ ' .')\n",
        "    return lyrics\n",
        "\n",
        "def regex_clean(lyrics):\n",
        "    lyrics = [x for x in lyrics if x!= ' .']\n",
        "    lyrics = [x for x in lyrics if x!= ' PLAYING .']\n",
        "    lyrics = [x for x in lyrics if not re.compile('Powered By(.*)').match(x) ]\n",
        "    lyrics = [re.sub('\\[(.*)\\]','',x) for x in lyrics]\n",
        "    lyrics = [re.sub('\\[(.*)\\.','',x) for x in lyrics]\n",
        "    lyrics = [re.sub('\\(','',x) for x in lyrics]\n",
        "    lyrics = [re.sub('\\)','',x) for x in lyrics]\n",
        "    lyrics = [re.sub('[wW]anna','want to',x) for x in lyrics]\n",
        "    lyrics = [re.sub('[gG]onna','going to',x) for x in lyrics]\n",
        "    lyrics = [re.sub('[gG]otta','have to',x) for x in lyrics]\n",
        "    lyrics = [re.sub(\"\\'em\",' them',x) for x in lyrics]\n",
        "    lyrics = [re.sub(\"\\'ll\",' will',x) for x in lyrics]\n",
        "    lyrics = [re.sub(\"\\'head\",'ahead',x) for x in lyrics]\n",
        "    lyrics = [re.sub(\"\\'bout\",'about',x) for x in lyrics]\n",
        "    lyrics = [re.sub('[wW]hatcha','what are you',x) for x in lyrics]\n",
        "    lyrics = [re.sub('[tT]ryna','trying to',x) for x in lyrics]\n",
        "    lyrics = [re.sub(\"\\'[cC]ause\",\" because \",x) for x in lyrics]\n",
        "    lyrics = [re.sub(\"\\'[rR]ound\",\"around\",x) for x in lyrics]\n",
        "    lyrics = [re.sub(\"ain't\",\"am not\",x) for x in lyrics]\n",
        "    lyrics = [re.sub(\"in\\'(\\s|\\,|\\.|\\?)\",\"ing \",x) for x in lyrics]\n",
        "    lyrics = [re.sub(\"on\\'(\\s|\\,|\\.|\\?)\",\"oing to \",x) for x in lyrics]\n",
        "    lyrics = [re.sub(\"o\\'s\",\"o is \",x) for x in lyrics]\n",
        "    lyrics = [re.sub(\"\\'re\",\" are\",x) for x in lyrics]\n",
        "    lyrics = [re.sub(\"\\'ve\",\" have\",x) for x in lyrics]\n",
        "    lyrics = [re.sub(\"\\'d\",\" would\",x) for x in lyrics]\n",
        "    lyrics = [re.sub(\"\\'(m|ma)\",\" am\",x) for x in lyrics]\n",
        "    lyrics = [re.sub('\\?(\\s*)\\.','?',x) for x in lyrics]\n",
        "    lyrics = [re.sub('\\,(\\s*)\\.',',',x) for x in lyrics]\n",
        "    lyrics = [re.sub('\\.(\\s*)\\.','.',x) for x in lyrics]\n",
        "    lyrics = [re.sub('\\.(\\s*)\\.','.',x) for x in lyrics]\n",
        "    lyrics = [re.sub('\\.(\\s*)\\.','.',x) for x in lyrics]\n",
        "    lyrics = [re.sub(\"\\'\",\"\",x) for x in lyrics]\n",
        "    return lyrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80c1cf00-b533-4017-a4d0-9a8109971e01",
      "metadata": {
        "id": "80c1cf00-b533-4017-a4d0-9a8109971e01"
      },
      "outputs": [],
      "source": [
        "# lyrics_df = pd.read_excel('/content/drive/MyDrive/Colab_Data/Lyrics_Cleaned.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80c1af7a-cb1a-4a28-ac64-ca0a263124a0",
      "metadata": {
        "id": "80c1af7a-cb1a-4a28-ac64-ca0a263124a0"
      },
      "outputs": [],
      "source": [
        "#removing the rows which have no lyrics\n",
        "lyrics_df = lyrics_df[lyrics_df['Lyrics'] != '[]'] \n",
        "lyrics_df = lyrics_df[lyrics_df['Genres'].notnull()]\n",
        "lyrics_df = lyrics_df[lyrics_df['Genres'].str.contains('Non-Music') == False]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "672ecb38-74ed-4c34-8322-4d0f8713f765",
      "metadata": {
        "id": "672ecb38-74ed-4c34-8322-4d0f8713f765"
      },
      "outputs": [],
      "source": [
        "lyrics_df.reset_index(drop = True,inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "290efb73-1cfd-4c1f-9fe2-939b14f6d8d7",
      "metadata": {
        "id": "290efb73-1cfd-4c1f-9fe2-939b14f6d8d7"
      },
      "outputs": [],
      "source": [
        "lyrics_df['Lyrics_Original'] = lyrics_df['Lyrics'].apply(lambda x: extract_lyrics(x))\n",
        "lyrics_df['Lyrics_Processed'] = lyrics_df['Lyrics_Original'].apply(lambda x: regex_clean(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72bd0c71-00f9-442b-9839-4f11d582a6f0",
      "metadata": {
        "id": "72bd0c71-00f9-442b-9839-4f11d582a6f0"
      },
      "outputs": [],
      "source": [
        "lyrics_df['Lyrics_Processed'] = lyrics_df['Lyrics_Processed'].apply(lambda x: \" \".join(x))\n",
        "# Dropping 'any' column\n",
        "# billboard_df.drop(['columnname'], inplace=True, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5152e77d-792d-4a67-a143-de20e8a5c94b",
      "metadata": {
        "id": "5152e77d-792d-4a67-a143-de20e8a5c94b"
      },
      "outputs": [],
      "source": [
        "splitlist =[]\n",
        "lyricsnew = []\n",
        "for j,lyric in enumerate(lyrics_df['Lyrics_Processed']):\n",
        "    splitlist = re.split('(?=[A-Z])',lyric)\n",
        "    for i,sentence in enumerate(splitlist):\n",
        "        splitlist[i]=sentence+\". \"\n",
        "    lyricsnew.append((\" \".join(splitlist)).strip().replace('. .','. ').replace('.  .','. ').replace('? .','? ').replace('?  .','? ').replace('! .','! ').replace('!  .','! ').replace('..','.').replace(',.',',').replace(', .',',').replace(',  .',','))#.replace(re.compile(),))\n",
        "    lyricsnew = [re.sub('(\\.)(.{1,15})(\\.)',r'\\1\\2',x) for x in lyricsnew]\n",
        "lyrics_df['Lyrics_Processed'] = lyricsnew\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language Detection and Filtering out Non-English Songs"
      ],
      "metadata": {
        "id": "lJyKW-6kmWFC"
      },
      "id": "lJyKW-6kmWFC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa060fdd-645d-4965-937a-06d04b3a26ab",
      "metadata": {
        "id": "aa060fdd-645d-4965-937a-06d04b3a26ab"
      },
      "outputs": [],
      "source": [
        "lyrics_df['Song_Language'] =  lyrics_df['Lyrics_Processed'].apply(lambda x: detect(x))\n",
        "lyrics_df=lyrics_df[lyrics_df['Song_Language']=='en']\n",
        "print('Number of songs remaining:', lyrics_df.shape[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Space removal"
      ],
      "metadata": {
        "id": "IsAFcNPbmjVJ"
      },
      "id": "IsAFcNPbmjVJ"
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_extra_spaces(lyric):\n",
        "    lyric = str(lyric)\n",
        "    while \"  \" in lyric:\n",
        "        lyric = lyric.replace(\"  \",\" \")\n",
        "    return lyric\n",
        "lyrics_df['Lyrics_Processed'] = lyrics_df['Lyrics_Processed'].apply(lambda x: remove_extra_spaces(x))"
      ],
      "metadata": {
        "id": "MliSL1UGQxck"
      },
      "id": "MliSL1UGQxck",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization (With stopword removal)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RowyP5kymm8T"
      },
      "id": "RowyP5kymm8T"
    },
    {
      "cell_type": "code",
      "source": [
        "lyrics_df['Lyrics_Tokenized'] =  lyrics_df['Lyrics_Processed'].apply(lambda x: word_tokenize(x))\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "# lyrics_df['Lyrics_Tokenized'] =  lyrics_df['Lyrics_Tokenized'].apply(lambda x: [word.lower() for word in x if word not in stop_words])"
      ],
      "metadata": {
        "id": "gQHn7_x6mvjO"
      },
      "id": "gQHn7_x6mvjO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS_TAGGING, NER, SENTIMENT ANALYSIS, EMOTION PREDICTION, WORD EMBEDDING with TFIDF and WORD2VEC"
      ],
      "metadata": {
        "id": "AhCz-DTwQ5as"
      },
      "id": "AhCz-DTwQ5as"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## POS Tagging"
      ],
      "metadata": {
        "id": "2pPDuSesjb07"
      },
      "id": "2pPDuSesjb07"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24553000-b0e7-4459-9e42-4012f90a092a",
      "metadata": {
        "id": "24553000-b0e7-4459-9e42-4012f90a092a"
      },
      "outputs": [],
      "source": [
        "lyrics_df['Lyrics_POS_Standard'] = lyrics_df['Lyrics_Tokenized'].apply(lambda x: pos_tag(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NER - Using NLP Default - Ne_chunk named entity recogniser"
      ],
      "metadata": {
        "id": "ugSbQ1u3jeAA"
      },
      "id": "ugSbQ1u3jeAA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4ebc962-35b9-49dc-b678-a1c0933cd64a",
      "metadata": {
        "id": "e4ebc962-35b9-49dc-b678-a1c0933cd64a"
      },
      "outputs": [],
      "source": [
        "lyrics_df['Lyrics_NER_Nechunk'] = lyrics_df['Lyrics_POS_Standard'].apply(lambda x: nltk.ne_chunk(x))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "st = StanfordNERTagger('/content/drive/MyDrive/Colab_Models/stanford-ner/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
        "\t\t\t\t\t   '/content/drive/MyDrive/Colab_Models/stanford-ner/stanford-ner.jar',\n",
        "\t\t\t\t\t   encoding='utf-8')\n",
        "lyrics_df[\"NER_Stanford\"] = lyrics_df[\"Lyrics_Tokenized\"].apply(lambda x: st.tag(x))\n",
        "lyrics_df[\"NER_Stanford\"] = lyrics_df[\"NER_Stanford\"].apply(lambda x: [word for word in x if word[1]!='O'])"
      ],
      "metadata": {
        "id": "VzLm7Zce8jM4"
      },
      "id": "VzLm7Zce8jM4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lyrics_df=lyrics_df.reset_index()\n",
        "lyrics_df[['index','Songs','Year','Primary_Artist','NER_Stanford']].to_excel('NERStandfordEntities.xlsx',header=True,index=False)"
      ],
      "metadata": {
        "id": "UXvGg-CCmKa0"
      },
      "id": "UXvGg-CCmKa0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d836231-8c31-44ca-b2f1-209b352355e5",
      "metadata": {
        "id": "9d836231-8c31-44ca-b2f1-209b352355e5"
      },
      "outputs": [],
      "source": [
        "def organise_entities(lyricNERresult):\n",
        "    entitylist = []\n",
        "    templist=[]\n",
        "    templist = re.findall(r'\\(.*\\)',str(lyricNERresult))\n",
        "    for entity in templist:\n",
        "        col1=\"\"\n",
        "        col1 = str(entity).replace('(','').replace(')','').split(' ')[0]\n",
        "        name = \"\"\n",
        "        for col in str(entity).replace('(','').replace(')','').split(' ')[1:]:\n",
        "            name+=col.split('/')[0]+\" \"\n",
        "            pos = col.split('/')[1]\n",
        "        entitylist.append([col1,name.strip(),pos.strip()])\n",
        "    return entitylist\n",
        "\n",
        "tagger = 'Nechunk' # StanfordNER\n",
        "if tagger == 'Nechunk':\n",
        "    lyrics_df['Lyrics_Entities'] = lyrics_df['Lyrics_NER_Nechunk'].apply(lambda x: organise_entities(x))\n",
        "\n",
        "def extract_entities(df,col,tagger = 'Nechunk'):\n",
        "    if tagger == 'Nechunk':\n",
        "        entitylist=[]\n",
        "        for colvalue in list(df[col].unique()):\n",
        "            templist=[]\n",
        "            for row in df[df[col]==colvalue]['Lyrics_Entities']:\n",
        "                if len(row)>0:\n",
        "                    for entitygroup in row:\n",
        "                        templist.append(entitygroup[1])\n",
        "            entitylist.append([colvalue,templist])\n",
        "    elif tagger == 'StanfordNER':\n",
        "        entitylist=[]\n",
        "        for colvalue in list(df[col].unique()):\n",
        "            templist=[]\n",
        "            for row in df[df[col]==colvalue]['NER_Stanford']:\n",
        "                if len(row)>0:\n",
        "                    for entitygroup in row:\n",
        "                        templist.append(entitygroup[0])\n",
        "            entitylist.append([colvalue,templist])\n",
        "    return entitylist\n",
        "\n",
        "allentitiesyearwise_StanfordNER=extract_entities(lyrics_df,'Year','StanfordNER')\n",
        "allentitiesyearwise=extract_entities(lyrics_df,'Year','Nechunk')\n",
        "allentitiesartistwise_StanfordNER=extract_entities(lyrics_df,'Primary_Artist','StanfordNER')\n",
        "allentitiesartistwise=extract_entities(lyrics_df,'Primary_Artist','Nechunk')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print([\"\\\\n row:\"+str(row) for row in lyrics_df['Lyrics_Entities']]) #lyrics [lyricofsong][numberofentitytuple][]"
      ],
      "metadata": {
        "id": "SDq2iAkiGKf8"
      },
      "id": "SDq2iAkiGKf8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c019c32b-9e4d-4e20-9c20-c9e8f785c2f9",
      "metadata": {
        "id": "c019c32b-9e4d-4e20-9c20-c9e8f785c2f9"
      },
      "outputs": [],
      "source": [
        "NERyearwise=pd.DataFrame(allentitiesyearwise,columns=['Year','EntityVocab'])\n",
        "NERyearwise.to_excel('EntitiesYearwise.xlsx',header=True,index=False)\n",
        "NERyearwise=pd.DataFrame(allentitiesyearwise_StanfordNER,columns=['Year','EntityVocab'])\n",
        "NERyearwise.to_excel('EntitiesYearwise_StanfordNER.xlsx',header=True,index=False)\n",
        "NERartistwise=pd.DataFrame(allentitiesartistwise,columns=['Primary_Artist','EntityVocab'])\n",
        "NERartistwise.to_excel('EntitiesArtistwise.xlsx',header=True,index=False)\n",
        "NERartistwise=pd.DataFrame(allentitiesartistwise_StanfordNER,columns=['Primary_Artist','EntityVocab'])\n",
        "NERartistwise.to_excel('EntitiesArtistwise_StanfordNER.xlsx',header=True,index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Emotion and Sentiment Analysis - Quantifying Emotions and Sentiment polarity in Song Lyrics"
      ],
      "metadata": {
        "id": "g65-SBPBjq8e"
      },
      "id": "g65-SBPBjq8e"
    },
    {
      "cell_type": "code",
      "source": [
        "emotion = pipeline('sentiment-analysis',model='arpanghoshal/EmoRoBERTa')"
      ],
      "metadata": {
        "id": "AYfdL_1p_KOY"
      },
      "id": "AYfdL_1p_KOY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Spacy Lemmatization for Topic Modelling\n",
        "# nlp = spacy.load(\"en_core_web_sm\",disable=[\"parser\",\"ner\"])\n",
        "# def lemmatization(lyric,allowed_postags=[\"NOUN\",\"ADJ\",\"VERB\",\"ADV\"]):\n",
        "#     finallyric = []\n",
        "#     lyric_out=\"\"\n",
        "#     doc = nlp(lyric)\n",
        "#     for token in doc:\n",
        "#         if token.pos_ in allowed_postags:\n",
        "#             finallyric.append(token.lemma_)\n",
        "#     if len(finallyric)>=450:\n",
        "#         lyric_out=\" \".join(finallyric[:450])\n",
        "#     else:\n",
        "#         lyric_out=\" \".join(finallyric)\n",
        "#     return lyric_out\n",
        "# lyrics_df['Lyrics_Processed_Emoroberta']=lyrics_df['Lyrics_Processed'].apply(lambda x:lemmatization(x))"
      ],
      "metadata": {
        "id": "olJGE4SFWoHh"
      },
      "id": "olJGE4SFWoHh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lyrics_df['Lyrics_Processed_Emoroberta'][0]\n",
        "# for lyric in lyrics_df['Lyrics_Processed_Emoroberta']:\n",
        "    # if len(lyric.split(\" \"))>=500:\n",
        "    # print(len(lyric.split(\" \")))"
      ],
      "metadata": {
        "id": "cfCtz61kbhEn"
      },
      "id": "cfCtz61kbhEn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lyrics_df['Lyrics_ROBERTA_Emotion'] = lyrics_df['Lyrics_Processed_Emoroberta'].apply(lambda x: emotion(x))\n",
        "lyrics_df['Lyrics_ROBERTA_Emotion'] = lyrics_df['Lyrics_Processed'].apply(lambda x: emotion(x))"
      ],
      "metadata": {
        "id": "oGc6LfdUANHV"
      },
      "id": "oGc6LfdUANHV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f22873be-4297-4800-9eb5-e7368135dc8d",
      "metadata": {
        "id": "f22873be-4297-4800-9eb5-e7368135dc8d"
      },
      "outputs": [],
      "source": [
        "from empath import Empath\n",
        "lexicon = Empath()\n",
        "emotiondict = pd.read_excel('/content/drive/MyDrive/Colab_Data/EmotionDict.xlsx')\n",
        "def quantify_emotions(lyric):\n",
        "    new_dict = {'Joy':0,'Sadness':0,'Anger':0,'Fear':0,'Surprise':0,'Neutral':0,'Love':0}\n",
        "    lexicon_dict = lexicon.analyze(lyric)\n",
        "    for key in lexicon_dict.keys():\n",
        "        for i in range(len(emotiondict)):\n",
        "            if(key == emotiondict['EmpathCategories'][i]):\n",
        "                new_dict[emotiondict['Emotiontag'][i]] += lexicon_dict[key]\n",
        "    normalizingconstant = len(lyric.split(\" \"))\n",
        "    for key in new_dict:\n",
        "        new_dict[key] = np.round(new_dict[key]/normalizingconstant*100,2)\n",
        "    return new_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee607b78-64ee-4667-8637-684aac43085b",
      "metadata": {
        "id": "ee607b78-64ee-4667-8637-684aac43085b"
      },
      "outputs": [],
      "source": [
        "lyrics_df['Lyrics_Emotional_Content'] = lyrics_df['Lyrics_Processed'].apply(lambda x: quantify_emotions(x))\n",
        "lyrics_df['Emotion_Joy'] = lyrics_df['Lyrics_Emotional_Content'].apply(lambda content: content['Joy'])\n",
        "lyrics_df['Emotion_Sadness'] = lyrics_df['Lyrics_Emotional_Content'].apply(lambda content: content['Sadness'])\n",
        "lyrics_df['Emotion_Anger'] = lyrics_df['Lyrics_Emotional_Content'].apply(lambda content: content['Anger'])\n",
        "lyrics_df['Emotion_Fear'] = lyrics_df['Lyrics_Emotional_Content'].apply(lambda content: content['Fear'])\n",
        "lyrics_df['Emotion_Surprise'] = lyrics_df['Lyrics_Emotional_Content'].apply(lambda content: content['Surprise'])\n",
        "lyrics_df['Emotion_Love'] = lyrics_df['Lyrics_Emotional_Content'].apply(lambda content: content['Love'])\n",
        "lyrics_df['Emotion_Neutral'] = lyrics_df['Lyrics_Emotional_Content'].apply(lambda content: content['Neutral'])\n",
        "lyrics_df['Emotion_Sentiment_Polarity'] = (lyrics_df['Emotion_Joy']+lyrics_df['Emotion_Love']-lyrics_df['Emotion_Anger']-lyrics_df['Emotion_Sadness']-lyrics_df['Emotion_Fear'])\n",
        "lyrics_df['Emotion_Sentiment']=np.where(lyrics_df['Emotion_Sentiment_Polarity']>=0.1,'POSITIVE',\n",
        "                                         np.where(lyrics_df['Emotion_Sentiment_Polarity']<=-0.1,'NEGATIVE',\n",
        "                                                  np.where((lyrics_df['Emotion_Sentiment_Polarity']>-0.1) & \n",
        "                                                           (lyrics_df['Emotion_Sentiment_Polarity']<0.1),'NEUTRAL','Unassigned')))\n",
        "lyrics_df.Emotion_Sentiment.value_counts().plot(kind='bar',title=\"Sentiment Analysis Results - From Emotions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac08d86e-79c7-4b49-8daf-784e89a7de25",
      "metadata": {
        "id": "ac08d86e-79c7-4b49-8daf-784e89a7de25"
      },
      "outputs": [],
      "source": [
        "# lyrics_df.to_excel('Lyrics_with_Emotions.xlsx',header=True,index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TEXTBLOB SENTIMENT ANALYSIS\n",
        "def get_polarity(text):\n",
        "    return TextBlob(text).sentiment.polarity\n",
        "lyrics_df['TextBlob_SentimentPolarity'] = lyrics_df['Lyrics_Processed'].apply(lambda x: get_polarity(x))\n",
        "lyrics_df['Textblob_Sentiment']=np.where(lyrics_df['TextBlob_SentimentPolarity']>=0.1,'POSITIVE',\n",
        "                                         np.where(lyrics_df['TextBlob_SentimentPolarity']<=-0.1,'NEGATIVE',\n",
        "                                                  np.where((lyrics_df['TextBlob_SentimentPolarity']>-0.1) & \n",
        "                                                           (lyrics_df['TextBlob_SentimentPolarity']<0.1),'NEUTRAL','Unassigned')))\n",
        "lyrics_df.Textblob_Sentiment.value_counts().plot(kind='bar',title=\"Sentiment Analysis Results - TextBlob\")"
      ],
      "metadata": {
        "id": "B4M2pGgnkERh"
      },
      "id": "B4M2pGgnkERh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#VADER SENTIMENT ANALYSIS\n",
        "# import SentimentIntensityAnalyzer class from vaderSentiment.vaderSentiment module.\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "# Create a SentimentIntensityAnalyzer object.\n",
        "sid_obj = SentimentIntensityAnalyzer()\n",
        "# polarity_scores method of SentimentIntensityAnalyzer object gives a sentiment dictionary.\n",
        "# which contains pos, neg, neu, and compound scores.\n",
        "lyrics_df['Vader_SentimentPolarity'] = lyrics_df['Lyrics_Processed'].apply(lambda x: sid_obj.polarity_scores(x))\n",
        "lyrics_df['VaderCompoundScore'] = lyrics_df['Vader_SentimentPolarity'].apply(lambda scores_dict: scores_dict['compound'])    \n",
        "lyrics_df['Vader_Sentiment']=np.where(lyrics_df['VaderCompoundScore']>=0.1,'POSITIVE',\n",
        "                                         np.where(lyrics_df['VaderCompoundScore']<=-0.1,'NEGATIVE',\n",
        "                                                  np.where((lyrics_df['VaderCompoundScore']>-0.1) & \n",
        "                                                           (lyrics_df['VaderCompoundScore']<0.1),'NEUTRAL','Unassigned')))\n",
        "lyrics_df.Vader_Sentiment.value_counts().plot(kind='bar',title=\"Sentiment Analysis Results - Vader\")\n",
        "lyrics_df.to_excel('Lyrics_with_Sentimotions.xlsx',header=True,index=False)"
      ],
      "metadata": {
        "id": "BNps3UKHhAX1"
      },
      "id": "BNps3UKHhAX1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gensim Word Vectorization using Word2Vec"
      ],
      "metadata": {
        "id": "qS1g_5Rlix-l"
      },
      "id": "qS1g_5Rlix-l"
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating an alternate deduplicated dataframe the song-artist combinations grossing across multiple years for word vectorization / topic modelling\n",
        "print(lyrics_df.columns)\n",
        "deduped_lyrics_df = lyrics_df.drop_duplicates(subset=['Songs','Primary_Artist'],keep='first')\n",
        "print(lyrics_df.shape,deduped_lyrics_df.shape)"
      ],
      "metadata": {
        "id": "s4Y3vlwPD1pG"
      },
      "id": "s4Y3vlwPD1pG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Word2vec implementation\n",
        "model = gensim.models.Word2Vec(deduped_lyrics_df['Lyrics_Tokenized'],min_count=5,workers=4,size=20,window=3,iter = 1000)\n",
        "#vocab size\n",
        "len(model.wv.vocab.keys())"
      ],
      "metadata": {
        "id": "OscUJFFUvy7v"
      },
      "id": "OscUJFFUvy7v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.vocab.keys()\n",
        "#below line to view model vocabulary\n",
        "# model.wv.vocab"
      ],
      "metadata": {
        "id": "wWeJXfoJ91a8"
      },
      "id": "wWeJXfoJ91a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#word2vector representation\n",
        "model.wv['great']"
      ],
      "metadata": {
        "id": "JPkClJHR98If"
      },
      "id": "JPkClJHR98If",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find similar words to the given word\n",
        "model.wv.most_similar('Queen')"
      ],
      "metadata": {
        "id": "jTmGOYzg-WDg"
      },
      "id": "jTmGOYzg-WDg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#words which don't match\n",
        "print(model.wv.doesnt_match('king driver kingdom queen'.split()))\n",
        "print(model.wv.doesnt_match('king kingdom house queen'.split()))\n",
        "print(model.wv.doesnt_match('king queen kingdom baby'.split()))"
      ],
      "metadata": {
        "id": "VGSCz7RX-5VI"
      },
      "id": "VGSCz7RX-5VI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#arithmetic operations\n",
        "model.most_similar(positive=['king','woman'], negative=['person'])[:100]"
      ],
      "metadata": {
        "id": "tt-pvFJv_Ueb"
      },
      "id": "tt-pvFJv_Ueb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving model\n",
        "model.wv.save('/content/drive/MyDrive/Colab_Models/word2vector-music20params_1000iter.bin')"
      ],
      "metadata": {
        "id": "wJNhL1hNADYz"
      },
      "id": "wJNhL1hNADYz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading model\n",
        "# model = gensim.models.Word2Vec.load('/content/drive/MyDrive/Colab_Models/word2vector-music192_100iter.bin')"
      ],
      "metadata": {
        "id": "IIPeSfa6p0qB"
      },
      "id": "IIPeSfa6p0qB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spacy Lemmatization for Word Vectorization and Topic Modelling"
      ],
      "metadata": {
        "id": "wHgZEaosilf7"
      },
      "id": "wHgZEaosilf7"
    },
    {
      "cell_type": "code",
      "source": [
        "#Spacy Lemmatization for Topic Modelling\n",
        "nlp = spacy.load(\"en_core_web_sm\",disable=[\"parser\",\"ner\"])\n",
        "def lemmatization(lyric, allowed_postags=[\"NOUN\",\"ADJ\",\"VERB\",\"ADV\"]):\n",
        "    finallyric = []\n",
        "    doc = nlp(lyric)\n",
        "    for token in doc:\n",
        "        if token.pos_ in allowed_postags:\n",
        "            finallyric.append(token.lemma_)\n",
        "    lyric_out=\" \".join(finallyric)\n",
        "    return lyric_out\n",
        "deduped_lyrics_df['SpacyLemmatized_Lyrics']=deduped_lyrics_df['Lyrics_Processed'].apply(lambda x:lemmatization(x))\n"
      ],
      "metadata": {
        "id": "Lqj18uTPA_Hz"
      },
      "id": "Lqj18uTPA_Hz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deduped_lyrics_df['SpacyLemmatized_Lyrics'][0]"
      ],
      "metadata": {
        "id": "LtTSmUIng494"
      },
      "id": "LtTSmUIng494",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF IDF Word Vectorization with KMeans Clustering applied to word vectors to discover closely related words"
      ],
      "metadata": {
        "id": "jOho2cediOFR"
      },
      "id": "jOho2cediOFR"
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(lowercase=True,max_features=100,max_df=0.8,min_df=5,ngram_range=(1,3),stop_words = \"english\")\n",
        "# vectors = vectorizer.fit_transform(deduped_lyrics_df['Lyrics_Processed'])\n",
        "vectors = vectorizer.fit_transform(deduped_lyrics_df['SpacyLemmatized_Lyrics'])\n",
        "feature_names=vectorizer.get_feature_names()\n",
        "dense = vectors.todense()\n",
        "denselist = dense.tolist()\n",
        "all_lyricwords = []\n",
        "for lyric in denselist:\n",
        "    x = 0\n",
        "    keywords = []\n",
        "    for word in lyric:\n",
        "        if word>0:\n",
        "            keywords.append(feature_names[x])\n",
        "        x+=1\n",
        "    all_lyricwords.append(keywords)\n",
        "# print(lyrics_df['Lyrics_Processed'][0])\n",
        "# print(all_lyricwords[0])\n",
        "\n",
        "k_topics = 10\n",
        "clusteringmodel = KMeans(n_clusters = k_topics, init =\"k-means++\", max_iter = 1000, n_init =1)\n",
        "clusteringmodel.fit(vectors)\n",
        "order_centroids = clusteringmodel.cluster_centers_.argsort()[:,::-1]\n",
        "terms = vectorizer.get_feature_names()\n",
        "indexlist=[]\n",
        "termlist=[]\n",
        "for i in range(k_topics):\n",
        "    indexlist.append(\"cluster_\"+str(i+1))\n",
        "    templist=[]\n",
        "    termlist.append([terms[index] for index in order_centroids[i,:20]])\n",
        "# print(np.shape(np.array(termlist).T))\n",
        "pd.DataFrame(data=np.array(termlist).T,columns = [indexlist]).to_csv('clustercenters.csv',header=True)"
      ],
      "metadata": {
        "id": "yawIoRgRp6kn"
      },
      "id": "yawIoRgRp6kn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting TFIDF - Kmeans topic clusters\n",
        "kmean_indices = clusteringmodel.fit_predict(vectors)\n",
        "pca = PCA(n_components = 2)\n",
        "scatter_plot_points = pca.fit_transform(vectors.toarray())\n",
        "colors = ['red','blue','green','yellow','magenta','purple','orange','violet','brown','black']\n",
        "x_axis = [p[0] for p in scatter_plot_points]\n",
        "y_axis = [p[1] for p in scatter_plot_points]\n",
        "fig,ax = plt.subplots(figsize = (50,50))\n",
        "ax.scatter(x_axis, y_axis, c = [colors[d] for d in kmean_indices])\n",
        "song_genres=[]\n",
        "for genrelist in list(deduped_lyrics_df['Genres']):\n",
        "    if genrelist[1:-1].split(',')[0]=='Pop':\n",
        "        if len(genrelist[1:-1].split(','))>1:\n",
        "            song_genres.append(genrelist[1:-1].split(',')[1])\n",
        "    else:\n",
        "        song_genres.append(genrelist[1:-1].split(',')[0])\n",
        "# song_artist_names=[entry.replace(\"$\",\"S\") for entry in song_artist_names]\n",
        "for i,txt in enumerate(song_genres):\n",
        "    ax.annotate(txt,(x_axis[i],y_axis[i]))\n",
        "plt.savefig(\"tfidf_kmeans_clusters.png\")"
      ],
      "metadata": {
        "id": "OlBNvloKx2MO"
      },
      "id": "OlBNvloKx2MO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic Modelling - Latent Dirichlet Allocation"
      ],
      "metadata": {
        "id": "b0nB05cPjGFa"
      },
      "id": "b0nB05cPjGFa"
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_words(lyric):\n",
        "    return gensim.utils.simple_preprocess(lyric,deacc=True)\n",
        "\n",
        "# deduped_lyrics_df['Gensim_Preprocessed_Lyrics'] = deduped_lyrics_df['SpacyLemmatized_Lyrics'].apply(lambda x: gen_words(x))\n",
        "deduped_lyrics_df['Gensim_Preprocessed_Lyrics'] = deduped_lyrics_df['Lyrics_Processed'].apply(lambda x: gen_words(x))\n",
        "# deduped_lyrics_df['Gensim_Preprocessed_Lyrics'][0][:20]"
      ],
      "metadata": {
        "id": "Zo5FDb2ScMtY"
      },
      "id": "Zo5FDb2ScMtY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_words = []\n",
        "for lyric in deduped_lyrics_df['Gensim_Preprocessed_Lyrics']:\n",
        "    data_words.append(lyric)"
      ],
      "metadata": {
        "id": "qFbBn9O6JAAW"
      },
      "id": "qFbBn9O6JAAW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_phrases = gensim.models.Phrases(data_words, min_count=2, threshold=5)\n",
        "trigram_phrases = gensim.models.Phrases(bigram_phrases[data_words], threshold=5)\n",
        "\n",
        "bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
        "trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return([bigram[doc] for doc in texts])\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return ([trigram[bigram[doc]] for doc in texts])\n",
        "\n",
        "data_bigrams = make_bigrams(data_words)\n",
        "data_bigrams_trigrams = make_trigrams(data_bigrams)\n",
        "data_bigrams_trigrams_bigrams = make_bigrams(data_bigrams_trigrams)\n",
        "data_bigrams_trigrams_bigrams_trigrams = make_trigrams(data_bigrams_trigrams_bigrams)\n",
        "data_bigrams_trigrams_bigrams_trigrams_bigrams = make_bigrams(data_bigrams_trigrams_bigrams_trigrams)\n",
        "data_bigrams_trigrams_bigrams_trigrams_bigrams_trigrams = make_trigrams(data_bigrams_trigrams_bigrams_trigrams_bigrams)\n",
        "print (data_bigrams_trigrams_bigrams_trigrams_bigrams_trigrams[0][0:20])\n",
        "\n",
        "print (len(list(data_bigrams_trigrams_bigrams_trigrams_bigrams)))"
      ],
      "metadata": {
        "id": "wzPfJIv7FN6-"
      },
      "id": "wzPfJIv7FN6-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TF IDF Unimportant word removal\n",
        "lyric2word = corpora.Dictionary(data_bigrams_trigrams_bigrams_trigrams_bigrams)\n",
        "x = data_bigrams_trigrams_bigrams_trigrams_bigrams_trigrams\n",
        "corpus=[]\n",
        "for lyric in data_bigrams_trigrams_bigrams_trigrams_bigrams_trigrams:\n",
        "    newlyric = lyric2word.doc2bow(lyric)\n",
        "    corpus.append(newlyric)\n",
        "print(corpus[:][:10])\n",
        "\n",
        "tfidf = TfidfModel(corpus, id2word=lyric2word)\n",
        "low_value = 0.08\n",
        "words=[]\n",
        "words_missing_in_tfidf=[]\n",
        "for i in range(0,len(corpus)):\n",
        "    bow=corpus[i]\n",
        "    low_value_words=[]\n",
        "    tfidf_ids = [id for id,value in tfidf[bow]]\n",
        "    bow_ids = [id for id,value in bow]\n",
        "    low_value_words = [id for id,value in tfidf[bow] if value<low_value]\n",
        "    drops=low_value_words+words_missing_in_tfidf\n",
        "    for item in drops:\n",
        "        words.append(lyric2word[item])\n",
        "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids]\n",
        "\n",
        "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
        "    corpus[i]=new_bow\n"
      ],
      "metadata": {
        "id": "-X8QU2-gFNob"
      },
      "id": "-X8QU2-gFNob",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model = gensim.models.ldamodel.LdaModel(\n",
        "    corpus = corpus, id2word = lyric2word, num_topics = 20, random_state=2, update_every=1, chunksize = 2000, passes = 10, alpha = \"symmetric\")"
      ],
      "metadata": {
        "id": "RFrYioV47crr"
      },
      "id": "RFrYioV47crr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, lyric2word, mds = \"mmds\", R=20)\n",
        "vis"
      ],
      "metadata": {
        "id": "h78a7oCWDLq6"
      },
      "id": "h78a7oCWDLq6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = deduped_lyrics_df['SpacyLemmatized_Lyrics']"
      ],
      "metadata": {
        "id": "pbox42SQSCaQ"
      },
      "id": "pbox42SQSCaQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model = BERTopic(language = 'English', calculate_probabilities=True)\n",
        "topics, _ = topic_model.fit_transform(docs)"
      ],
      "metadata": {
        "id": "_Yu_4rWHhF3i"
      },
      "id": "_Yu_4rWHhF3i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_freq = topic_model.get_topic_freq()\n",
        "number_of_outliers = topic_freq['Count'][topic_freq['Topic']==-1].iloc[0]\n",
        "print(f\"{number_of_outliers} songs have not been assigned to any topic\")\n",
        "print(f\"The other {topic_freq['Count'].sum()-number_of_outliers} songs are talking about {topic_freq['Topic'].shape[0]-1} topics\")"
      ],
      "metadata": {
        "id": "G2dnsEJWjspT"
      },
      "id": "G2dnsEJWjspT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# topic_freq.head(15)\n",
        "topic_model.get_topic(topic_freq['Topic'].iloc[1])"
      ],
      "metadata": {
        "id": "1sa5hJAKkxde"
      },
      "id": "1sa5hJAKkxde",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topicslist = [topic_model.get_topic(topic_freq['Topic'].iloc[int(topic+1)]) for topic in range(topic_freq.shape[0]-1)]"
      ],
      "metadata": {
        "id": "5Gi-zDDllHvc"
      },
      "id": "5Gi-zDDllHvc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topicslist = [list(x) for x in zip(*topicslist)]\n",
        "for i in range(len(topicslist)):\n",
        "    for j in range(9):\n",
        "        topicslist[i][j]=(topicslist[i][j][0],np.round(topicslist[i][j][1],4))\n",
        "# topicslist"
      ],
      "metadata": {
        "id": "gVP_6UzYmWAO"
      },
      "id": "gVP_6UzYmWAO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_df=pd.DataFrame(topicslist, columns=[f'topic_{x+1}' for x in range(topic_freq.shape[0]-1)])"
      ],
      "metadata": {
        "id": "s-vKvjrnldk1"
      },
      "id": "s-vKvjrnldk1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_df.head(int(topic_df.shape[0]))"
      ],
      "metadata": {
        "id": "s4fAJIiynEia"
      },
      "id": "s4fAJIiynEia",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Song repetitive check using Lempel Ziv Markov Algorithm"
      ],
      "metadata": {
        "id": "BqX2fgfR2zZj"
      },
      "id": "BqX2fgfR2zZj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Song Repetitiveness\n",
        "import lzma\n",
        "import os\n",
        "\n",
        "\n",
        "lyrics_df = pd.read_excel('lyrics_dataset_final.xlsx')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "lyrics_df[\"Pre_compression\"] = np.nan\n",
        "lyrics_df[\"Post_compression\"] = np.nan\n",
        "for i in range(len(lyrics_df)):\n",
        "    \n",
        "    data = lyrics_df[\"Lyrics_Processed\"][i]\n",
        "    with lzma.open(\"file.lzma\", \"wt\") as f:\n",
        "        f.write(data)\n",
        "\n",
        "    with open(\"file.txt\", \"w\") as f:\n",
        "        f.write(data)   \n",
        "        \n",
        "    lyrics_df[\"Pre_compression\"][i]  = os.path.getsize(\"file.txt\") \n",
        "    lyrics_df[\"Post_compression\"][i]  = os.path.getsize(\"file.lzma\") \n",
        "    \n",
        "lyrics_df[\"Ratio\"] = 100*lyrics_df[\"Post_compression\"]/lyrics_df[\"Pre_compression\"]    \n",
        "lyrics_df.sort_values(by = \"Ratio\",ascending=False)\n",
        "\n",
        "lyrics_df = lyrics_df.sort_values(by = \"Ratio\",ascending=True)[:15].drop_duplicates(subset = [\"Artists\",\"Songs\"])[:10]\n",
        "lyrics_df[[\"Artists\",\"Songs\",\"Ratio\"]].to_excel(r'repeated_lyrics.xlsx',index = False)"
      ],
      "metadata": {
        "id": "NkCkddKanqrA"
      },
      "id": "NkCkddKanqrA",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "IRTM_PA_AmitJ_AdvaithJ_May2022.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "YcL4oKTXNg2c",
        "XNFoIo3_m-7J",
        "k03DDACunJ33",
        "lJyKW-6kmWFC",
        "IsAFcNPbmjVJ",
        "RowyP5kymm8T",
        "2pPDuSesjb07",
        "ugSbQ1u3jeAA",
        "g65-SBPBjq8e",
        "qS1g_5Rlix-l",
        "wHgZEaosilf7",
        "jOho2cediOFR",
        "b0nB05cPjGFa",
        "BqX2fgfR2zZj"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}